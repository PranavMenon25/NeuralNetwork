{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc896f89-14cc-4304-917a-e29e22c319ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef1b7ad-b2e2-4a2b-afe6-228cbf71d71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b42ce7f-10e4-4e62-8da1-906d630d45e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47fd873-d62a-43be-983c-45f767b89b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary of characters and mappints to / from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783949d8-b1cc-42ca-9e99-b364ba9f66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataSet\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words[:]:\n",
    "      context = [0] * block_size\n",
    "      for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "      \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 =int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4c531f-e901-4684-8abd-354a1fa9b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_emb), generator = g)\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden), generator = g) * ((5/3 ) / (n_emb * block_size ** 0.5))\n",
    "b1 = torch.randn((n_hidden), generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.01\n",
    "b2 = torch.randn((vocab_size), generator = g) * 0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b1, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149c2e7f-0141-4b65-bd3a-5415ba4c55ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2050)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(1000) * 0.2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1c6fde-05e5-4992-bff9-ddb661370a05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hpreact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hpreact\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hpreact' is not defined"
     ]
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfea35ef-9649-45dc-90ef-12853d34ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.0977\n",
      "  20000/ 200000: 2.3415\n",
      "  30000/ 200000: 2.3992\n",
      "  40000/ 200000: 2.0277\n",
      "  50000/ 200000: 2.3060\n",
      "  60000/ 200000: 2.4402\n",
      "  70000/ 200000: 2.0833\n",
      "  80000/ 200000: 2.3971\n",
      "  90000/ 200000: 2.1310\n",
      " 100000/ 200000: 1.7951\n",
      " 110000/ 200000: 2.4099\n",
      " 120000/ 200000: 1.9640\n",
      " 130000/ 200000: 2.3886\n",
      " 140000/ 200000: 2.3378\n",
      " 150000/ 200000: 2.2366\n",
      " 160000/ 200000: 1.8501\n",
      " 170000/ 200000: 1.8672\n",
      " 180000/ 200000: 2.0711\n",
      " 190000/ 200000: 1.8476\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "# Batch normalization alternatives -> Group normalization. \n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * ((hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim = True)) + bnbias\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass \n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters: \n",
    "    p.data += -(lr * p.grad)\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d0a8a6-bf2f-4f6a-9ee4-37762d7c7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass through the training set\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean and standard deviation once\n",
    "    bnmean = hpreact.mean(0, keepdim = True)\n",
    "    bnstd = hpreact.std(0, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac8e5892-3257-4213-9671-a10552cb97a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor(1/27.0).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe3b4598-3943-4933-a96a-dbfd1feca1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0600664615631104\n",
      "val 2.106653928756714\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': {Xtr, Ytr},\n",
    "        'val': {Xdev, Ydev},\n",
    "        'test': {Xte, Yte},\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * ((hpreact - bnmean) / bnstd) + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "910067de-e8c7-42b8-81c1-8595f50c43a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrhajimsadhitevmndhaylah.\n",
      "zhyrstendrlegradleydhelitzell.\n",
      "pplrodhis.\n",
      "quanassephultz.\n",
      "kasciyshubvrghhirgestin.\n",
      "jostlyssngbertry.\n",
      "bakyvder.\n",
      "ystuf.\n",
      "qhaylah.\n",
      "quonnyshahmandalysssufpzokhlejusmfrcebvanvevodtelladirferrohhidgsptin.\n",
      "shaulyss.\n",
      "hurdrs.\n",
      "malkirffrv.\n",
      "khwrlla.\n",
      "drthbetht.\n",
      "khasirvabdityt.\n",
      "dvassivveahnd.\n",
      "ryephnn.\n",
      "dhxavonndkylckennakatimvandridhb.\n",
      "daphvarlquinashpauduisldelltva.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        # Sample from the dist.\n",
    "        ix = torch.multinomial(probs, num_samples = 1, generator = g).item()\n",
    "        # shift the context window\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2cf83b4a-2e69-4de9-ab0c-cdd49c6bf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4aa77114-857e-469a-b7bf-2a2624559580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47551\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias = True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g)/fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean)/torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        # print(self.out)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "\n",
    "C = torch.Generator().manual_seed(2147483637)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden), BatchNorm(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), BatchNorm(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), BatchNorm(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), BatchNorm(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden), BatchNorm(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size), BatchNorm(vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Makes the prediction of last layer less\n",
    "    layers[-1].gamma *= 0.1\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7a09884-5a4c-4a86-8c3c-11f9df706853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000 : 3.2938\n",
      "  10000/ 200000 : 2.1750\n",
      "  20000/ 200000 : 2.2692\n",
      "  30000/ 200000 : 2.2090\n",
      "  40000/ 200000 : 2.3630\n",
      "  50000/ 200000 : 2.1190\n",
      "  60000/ 200000 : 2.2416\n",
      "  70000/ 200000 : 2.0860\n",
      "  80000/ 200000 : 1.9099\n",
      "  90000/ 200000 : 2.0437\n",
      " 100000/ 200000 : 2.2271\n",
      " 110000/ 200000 : 1.9269\n",
      " 120000/ 200000 : 2.0295\n",
      " 130000/ 200000 : 2.1561\n",
      " 140000/ 200000 : 2.2428\n",
      " 150000/ 200000 : 2.2365\n",
      " 160000/ 200000 : 2.0979\n",
      " 170000/ 200000 : 2.0917\n",
      " 180000/ 200000 : 1.6993\n",
      " 190000/ 200000 : 1.9925\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.1 if i < 10000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d} : {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    # break\n",
    "# In principle a large linear function is just a single neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e017de93-86af-4c35-97de-f2bfd17dec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_2776\\4264691136.py:25: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  xvar = x.var(0, keepdim=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(x, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sample from the dist.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, generator \u001b[38;5;241m=\u001b[39m g)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# shift the context window\u001b[39;00m\n\u001b[0;32m     15\u001b[0m context \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m [ix]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        x = emb.view(emb.shape[0], -1)\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        probs = F.softmax(x, dim = 1)\n",
    "        # Sample from the dist.\n",
    "        ix = torch.multinomial(probs, num_samples = 1, generator = g).item()\n",
    "        # shift the context window\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16b9c2-1b09-43ff-830f-12733f192aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87c04a-a1b5-47f4-b25d-7ef2bbcf7861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
